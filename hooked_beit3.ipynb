{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'torchscale' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/William-Chittavong/torchscale.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT OFF: hook has wrong path for imports. change them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBEiT3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BEiT3\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(BEiT3\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames)\n",
      "File \u001b[0;32m~/Documents/GitHub/torchscale/torchscale/model/BEiT3.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchitecture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mencoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoder\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     PositionalEmbedding,\n\u001b[1;32m     11\u001b[0m     TextEmbedding,\n\u001b[1;32m     12\u001b[0m     VisionEmbedding,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiway_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MutliwayEmbedding\n",
      "File \u001b[0;32m~/Documents/GitHub/torchscale/torchscale/architecture/encoder.py:12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfairscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m checkpoint_wrapper, wrap\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marchitecture\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_bert_params\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdroppath\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DropPath\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeedforward_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeedForwardNetwork, make_experts\n",
      "File \u001b[0;32m~/Documents/GitHub/torchscale/torchscale/architecture/utils.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) 2022 Microsoft\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Licensed under The MIT License [see LICENSE for details]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultihead_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiheadAttention\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchscale\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomponent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiway_network\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiwayNetwork\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_bert_params\u001b[39m(module):\n",
      "File \u001b[0;32m~/Documents/GitHub/torchscale/torchscale/component/multihead_attention.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookManager\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meinops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hook'"
     ]
    }
   ],
   "source": [
    "from torchscale.model.BEiT3 import BEiT3\n",
    "print(BEiT3.__init__.__code__.co_varnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/william/Documents/GitHub/torchscale/torchscale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/envs/beit3/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd torchscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n",
    "\n",
    "from torchscale.model.BEiT3 import BEiT3\n",
    "from torchscale.architecture.config import EncoderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(tensor, mean=0., std=1.):\n",
    "    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n",
    "\n",
    "\n",
    "def get_base_config(\n",
    "        img_size=224, patch_size=16, drop_path_rate=0, \n",
    "        checkpoint_activations=None, mlp_ratio=4, vocab_size=64010, **kwargs\n",
    "):\n",
    "    return EncoderConfig(\n",
    "        img_size=img_size, patch_size=patch_size, vocab_size=vocab_size, multiway=True, \n",
    "        layernorm_embedding=False, normalize_output=True, no_output_layer=True, \n",
    "        drop_path_rate=drop_path_rate, encoder_embed_dim=768, encoder_attention_heads=12, \n",
    "        encoder_ffn_embed_dim=int(768 * mlp_ratio), encoder_layers=12, \n",
    "        checkpoint_activations=checkpoint_activations, \n",
    "    )\n",
    "\n",
    "\n",
    "def get_large_config(\n",
    "        img_size=224, patch_size=16, drop_path_rate=0, \n",
    "        checkpoint_activations=None, mlp_ratio=4, vocab_size=64010, **kwargs\n",
    "):\n",
    "    return EncoderConfig(\n",
    "        img_size=img_size, patch_size=patch_size, vocab_size=vocab_size, multiway=True, \n",
    "        layernorm_embedding=False, normalize_output=True, no_output_layer=True, \n",
    "        drop_path_rate=drop_path_rate, encoder_embed_dim=1024, encoder_attention_heads=16, \n",
    "        encoder_ffn_embed_dim=int(1024 * mlp_ratio), encoder_layers=24, \n",
    "        checkpoint_activations=checkpoint_activations, \n",
    "    )\n",
    "\n",
    "\n",
    "class BEiT3Wrapper(nn.Module):\n",
    "    def __init__(self, args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.beit3 = BEiT3(args)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return self.beit3.encoder.num_layers\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token', 'beit3.encoder.embed_positions.A.weight', 'beit3.vision_embed.cls_token', 'logit_scale'}\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.distributed as dist\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "class GatherLayer(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gather tensors from all workers with support for backward propagation:\n",
    "    This implementation does not cut the gradients as torch.distributed.all_gather does.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]\n",
    "        dist.all_gather(output, x)\n",
    "        return tuple(output)\n",
    "    @staticmethod\n",
    "    def backward(ctx, *grads):\n",
    "        all_gradients = torch.stack(grads)\n",
    "        dist.all_reduce(all_gradients)\n",
    "        return all_gradients[dist.get_rank()]\n",
    "\n",
    "\n",
    "def gather_features(\n",
    "        image_features,\n",
    "        text_features,\n",
    "):\n",
    "    gathered_image_features = GatherLayer.apply(image_features)\n",
    "    gathered_text_features = GatherLayer.apply(text_features)\n",
    "    all_image_features = torch.cat(gathered_image_features)\n",
    "    all_text_features = torch.cat(gathered_text_features)\n",
    "\n",
    "    return all_image_features, all_text_features\n",
    "\n",
    "\n",
    "# The implementation code is modified from open_clip (https://github.com/mlfoundations/open_clip.git)\n",
    "class ClipLoss(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cache_labels=False,\n",
    "            rank=0,\n",
    "            world_size=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cache_labels = cache_labels\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "        # cache state\n",
    "        self.prev_num_logits = 0\n",
    "        self.labels = {}\n",
    "\n",
    "    def forward(self, image_features, text_features, logit_scale):\n",
    "        device = image_features.device\n",
    "        if self.world_size > 1:\n",
    "            all_image_features, all_text_features = gather_features(\n",
    "                image_features, text_features\n",
    "            )\n",
    "\n",
    "            logits_per_image = logit_scale * image_features @ all_text_features.T\n",
    "            logits_per_text = logit_scale * text_features @ all_image_features.T\n",
    "        else:\n",
    "            logits_per_image = logit_scale * image_features @ text_features.T\n",
    "            logits_per_text = logit_scale * text_features @ image_features.T\n",
    "\n",
    "        # calculated ground-truth and cache if enabled\n",
    "        num_logits = logits_per_image.shape[0]\n",
    "        if self.prev_num_logits != num_logits or device not in self.labels:\n",
    "            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n",
    "            if self.world_size > 1:\n",
    "                labels = labels + num_logits * self.rank\n",
    "            if self.cache_labels:\n",
    "                self.labels[device] = labels\n",
    "                self.prev_num_logits = num_logits\n",
    "        else:\n",
    "            labels = self.labels[device]\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_image, labels) +\n",
    "            F.cross_entropy(logits_per_text, labels)\n",
    "            ) / 2\n",
    "        return total_loss, logits_per_image, logits_per_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BEiT3ForRetrieval(BEiT3Wrapper):\n",
    "    def __init__(\n",
    "            self, \n",
    "            args,\n",
    "            hook,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super(BEiT3ForRetrieval, self).__init__(args=args)\n",
    "        self.hook_manager = hook\n",
    "        embed_dim = args.encoder_embed_dim\n",
    "        self.language_head = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.vision_head = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.language_head.apply(self._init_weights)\n",
    "        self.vision_head.apply(self._init_weights)\n",
    "        self.criterion = ClipLoss(\n",
    "            rank=get_rank(), \n",
    "            world_size=get_world_size(), \n",
    "        )\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "    def forward(self, image=None, text_description=None, padding_mask=None, only_infer=False, **kwargs):\n",
    "        if image is not None:\n",
    "            outputs = self.beit3(\n",
    "                textual_tokens=None, \n",
    "                visual_tokens=image, \n",
    "                text_padding_position=None,\n",
    "                hook = self.hook_manager \n",
    "            )\n",
    "            x = outputs[\"encoder_out\"]\n",
    "            vision_cls = self.vision_head(x[:, 0, :])\n",
    "            vision_cls = F.normalize(vision_cls, dim=-1)\n",
    "        else:\n",
    "            vision_cls = None\n",
    "\n",
    "        if text_description is not None:\n",
    "            outputs = self.beit3(\n",
    "                textual_tokens=text_description, \n",
    "                visual_tokens=None, \n",
    "                text_padding_position=padding_mask, \n",
    "            )\n",
    "            x = outputs[\"encoder_out\"]\n",
    "            language_cls = self.language_head(x[:, 0, :])\n",
    "            language_cls = F.normalize(language_cls, dim=-1)\n",
    "        else:\n",
    "            language_cls = None\n",
    "        \n",
    "        if only_infer:\n",
    "            return vision_cls, language_cls\n",
    "        else:\n",
    "            loss, logits_per_image, logits_per_text = self.criterion(\n",
    "                vision_cls, language_cls, self.logit_scale.exp())\n",
    "            return loss, vision_cls, language_cls\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_beit3_retrieval_model(model_size='base',hook_manager= None, img_size=224, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a BEiT3 model for retrieval tasks.\n",
    "    \n",
    "    Args:\n",
    "    model_size (str): 'base' or 'large'\n",
    "    img_size (int): Image size (assuming square images)\n",
    "    **kwargs: Additional arguments to pass to the model\n",
    "    \n",
    "    Returns:\n",
    "    BEiT3ForRetrieval: The created model\n",
    "    \"\"\"\n",
    "    if model_size not in ['base', 'large']:\n",
    "        raise ValueError(\"model_size must be either 'base' or 'large'\")\n",
    "    \n",
    "    if model_size == 'base':\n",
    "        args = get_base_config(img_size=img_size, **kwargs)\n",
    "    else:  # large\n",
    "        args = get_large_config(img_size=img_size, **kwargs)\n",
    "   \n",
    "    \n",
    "    model = BEiT3ForRetrieval(args, hook=hook_manager, **kwargs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why is kwargs wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = HookManager()\n",
    "model_size = \"base\"\n",
    "img_size = 224\n",
    "\n",
    "retrieve_model = create_beit3_retrieval_model(model_size='base',hook_manager= hook, img_size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BEiT3ForRetrieval(\n",
       "  (beit3): BEiT3(\n",
       "    (text_embed): TextEmbedding(64010, 768)\n",
       "    (vision_embed): VisionEmbedding(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (encoder): Encoder(\n",
       "      (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "      (embed_positions): MutliwayEmbedding(\n",
       "        (A): PositionalEmbedding(199, 768)\n",
       "        (B): PositionalEmbedding(1024, 768)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x EncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (k_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (B): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (v_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (B): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (q_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (B): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (out_proj): MultiwayNetwork(\n",
       "              (A): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (B): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (inner_attn_ln): MultiwayNetwork(\n",
       "              (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (self_attn_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "          (ffn): MultiwayNetwork(\n",
       "            (A): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (B): FeedForwardNetwork(\n",
       "              (activation_dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (dropout_module): Dropout(p=0.0, inplace=False)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (ffn_layernorm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (final_layer_norm): MultiwayNetwork(\n",
       "            (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): MultiwayNetwork(\n",
       "        (A): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (B): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (language_head): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (vision_head): Linear(in_features=768, out_features=768, bias=False)\n",
       "  (criterion): ClipLoss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRSLogger(object):\n",
    "    def __init__(self, model, embed_dim,device):\n",
    "        self.current_layer = 0\n",
    "        self.device = device\n",
    "        self.attentions = []\n",
    "        self.mlps = []\n",
    "        self.post_ln_std = None\n",
    "        self.post_ln_mean = None\n",
    "        self.model = model\n",
    "        self.vision_head = torch.nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_attentions(self, ret):\n",
    "        bias_term = self.model.encoder.layers[self.current_layer].self_attn.out_proj.bias\n",
    "\n",
    "        self.current_layer += 1\n",
    "        return_value = ret[:, 0].detach().cpu()\n",
    "        self.attentions.append(\n",
    "            return_value\n",
    "            + bias_term[np.newaxis, np.newaxis, np.newaxis].cpu()\n",
    "            / (return_value.shape[1] * return_value.shape[2])\n",
    "        )  # [b, n, h, d]\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_mlps(self, ret):\n",
    "        self.mlps.append(ret[:, 0].detach().cpu())  # [b, d]\n",
    "        return ret\n",
    "\n",
    " \n",
    "    @torch.no_grad()\n",
    "    def log_post_ln_mean(self, ret):\n",
    "        self.post_ln_mean = ret.detach().cpu()  # [b, 1]\n",
    "        return ret\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def log_post_ln_std(self, ret):\n",
    "        self.post_ln_std = ret.detach().cpu()  # [b, 1]\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def register_hooks(self):\n",
    "        self.model.hook_manager.register(\n",
    "            \"encoder.layer.*.self_attn.out_proj_post*\",\n",
    "            self.compute_attentions\n",
    "        )\n",
    "       \n",
    "        self.model.hook_manager.register(\n",
    "            \"encoder.layer.not_moe.ffn.fc2_post\",\n",
    "            self.compute_mlps\n",
    "        )\n",
    "        \n",
    "        #MOE FFNs\n",
    "        self.model.hook_manager.register(\n",
    "            \"encoder.layer.moe.expert.*.ffn.fc2_post\",\n",
    "            self.compute_mlps\n",
    "        )\n",
    "        \n",
    "        # IS THE THING BELOW needed? why is the layer norm before\n",
    "        # the transformer resblocks included in the mlps?\n",
    "        \n",
    "        # LN before the other encoder layers but self attn already happened\n",
    "        # what about layernorm in the forward embedding? ah nvm, its before self attn\n",
    "        self.model.hook_manager.register(\n",
    "            \"encoder.layer.0.self_attn_layer_norm.*.ln_post\",self.compute_mlps\n",
    "        )\n",
    "\n",
    "        #after final layer's layer norm. \n",
    "        self.model.hook_manager.register(\n",
    "            f\"encoder.layer_norm_post.mean\",\n",
    "            self.log_post_ln_mean\n",
    "        )\n",
    "        \n",
    "        self.model.hook_manager.register(\n",
    "            f\"encoder.layer_norm_post.sqrt_var\",\n",
    "            self.log_post_ln_std\n",
    "        )\n",
    "\n",
    "\n",
    "    def _normalize_mlps(self):\n",
    "        len_intermediates = self.attentions.shape[1] + self.mlps.shape[1]\n",
    "        # This is just the normalization layer:\n",
    "        mean_centered = (\n",
    "            self.mlps\n",
    "            - self.post_ln_mean[:, :, np.newaxis].to(self.device) / len_intermediates\n",
    "        )\n",
    "        weighted_mean_centered = (\n",
    "            self.model.beit3.encoder.layernorm.B.weight.detach().to(self.device) * mean_centered\n",
    "\n",
    "        )\n",
    "        weighted_mean_by_std = weighted_mean_centered / self.post_ln_std[\n",
    "            :, :, np.newaxis, np.newaxis, np.newaxis\n",
    "        ].to(self.device)\n",
    "        bias_term = self.model.beit3.encoder.layernorm.B.bias.detach().to(self.device) / (\n",
    "            len_intermediates \n",
    "        )\n",
    "        post_ln = weighted_mean_by_std + bias_term\n",
    "        return post_ln @ self.model.beit3.encoder.output_projection.detach().to(self.device)\n",
    "\n",
    "    def _normalize_attentions(self):\n",
    "        len_intermediates = self.attentions.shape[1] + self.mlps.shape[1]  # 2*l + 1\n",
    "        normalization_term = (\n",
    "            self.attentions.shape[2] * self.attentions.shape[3]\n",
    "        )  # n * h\n",
    "        # This is just the normalization layer:\n",
    "        mean_centered = self.attentions - self.post_ln_mean[\n",
    "            :, :, np.newaxis, np.newaxis, np.newaxis\n",
    "        ].to(self.device) / (len_intermediates * normalization_term)\n",
    "        \n",
    "        weighted_mean_centered = (\n",
    "            self.model.beit3.encoder.layernorm.B.weight.detach().to(self.device) * mean_centered\n",
    "\n",
    "        )\n",
    "        weighted_mean_by_std = weighted_mean_centered / self.post_ln_std[\n",
    "            :, :, np.newaxis, np.newaxis, np.newaxis\n",
    "        ].to(self.device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        bias_term = self.model.beit3.encoder.layernorm.B.bias.detach().to(self.device) / (\n",
    "            len_intermediates * normalization_term\n",
    "        )\n",
    "        \n",
    "        post_ln = weighted_mean_by_std + bias_term\n",
    "        return post_ln @ self.model.beit3.encoder.output_projection.detach().to(self.device)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def finalize(self):\n",
    "        \"\"\"We calculate the post-ln scaling, project it and normalize by the last norm.\"\"\"\n",
    "        self.attentions = torch.stack(self.attentions, axis=1).to(\n",
    "            self.device\n",
    "        )  # [b, l, n, h, d]\n",
    "        self.mlps = torch.stack(self.mlps, axis=1).to(self.device)  # [b, l + 1, d]\n",
    "        projected_attentions = self._normalize_attentions()\n",
    "        projected_mlps = self._normalize_mlps()\n",
    "        \n",
    "        vision_cls_proj_attn = self.vision_head(projected_attentions)\n",
    "        vision_cls_proj_mlps = self.vision_head(projected_mlps)\n",
    "        \n",
    "        attentions = F.normalize(vision_cls_proj_attn)\n",
    "        mlps = F.normalize(vision_cls_proj_mlps)\n",
    "        \n",
    "        return(\n",
    "            attentions,mlps\n",
    "        )\n",
    "\n",
    "    def reinit(self):\n",
    "        self.current_layer = 0\n",
    "        self.attentions = []\n",
    "        self.mlps = []\n",
    "        self.post_ln_mean = None\n",
    "        self.post_ln_std = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def hook_prs_logger(model, embed_dim, device):\n",
    "    \"\"\"Hooks a projected residual stream logger to the model.\"\"\"\n",
    "    prs = PRSLogger(model, embed_dim, device)\n",
    "    prs.register_hooks()\n",
    "    return prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ckpt from https://github.com/addf400/files/releases/download/beit3/beit3_base_itc_patch16_224.pth\n",
      "Load state_dict by model_key = model\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "from unilm.beit3.utils import load_model_and_may_interpolate\n",
    "\n",
    "\n",
    "# Load and transform the image\n",
    "#image_path = \"/content/tiny-imagenet-200/val/images/val_0.JPEG\"\n",
    "\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer(\"beit3.spm\")\n",
    "\n",
    "image_path = \"/home/william/project/images/catdog.png\"\n",
    "image = Image.open(image_path)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to the size expected by the model\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "])\n",
    "\n",
    "img_tensor = transform(image).unsqueeze(0).requires_grad_(True)\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "checkpoint_path = \"https://github.com/addf400/files/releases/download/beit3/beit3_base_patch16_224.pth\"\n",
    "\n",
    "image_text_contrastive_checkpoint = \"https://github.com/addf400/files/releases/download/beit3/beit3_base_itc_patch16_224.pth\"\n",
    "\n",
    "\n",
    "# Load the checkpoint into vqa_fixed\n",
    "#load_model_and_may_interpolate(checkpoint_path, vqa_model, model_key='model', model_prefix='')\n",
    "\n",
    "load_model_and_may_interpolate(image_text_contrastive_checkpoint, retrieve_model, model_key='model', model_prefix='')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_model.to(device)\n",
    "retrieve_model.eval()\n",
    "\n",
    "encoder_embed_dim = 768\n",
    "\n",
    "prs = hook_prs_logger(retrieve_model,encoder_embed_dim, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BEiT3ForRetrieval' object has no attribute 'hook'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mretrieve_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     attentions,mlps \u001b[38;5;241m=\u001b[39m prs\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/envs/beit3/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/beit3/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mBEiT3ForRetrieval.forward\u001b[0;34m(self, image, text_description, padding_mask, only_infer, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, text_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, padding_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, only_infer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeit3(\n\u001b[1;32m     24\u001b[0m             textual_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \n\u001b[1;32m     25\u001b[0m             visual_tokens\u001b[38;5;241m=\u001b[39mimage, \n\u001b[1;32m     26\u001b[0m             text_padding_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m---> 27\u001b[0m             hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook\u001b[49m \n\u001b[1;32m     28\u001b[0m         )\n\u001b[1;32m     29\u001b[0m         x \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_out\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     30\u001b[0m         vision_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_head(x[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/envs/beit3/lib/python3.10/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BEiT3ForRetrieval' object has no attribute 'hook'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    retrieve_model(image=img_tensor)\n",
    "    attentions,mlps = prs.finalize()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beit3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
